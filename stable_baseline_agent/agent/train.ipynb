{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bomberman.Environment import Environnement\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from DQN_Agent import BuffedDQNAgent\n",
    "\n",
    "class Trainer():\n",
    "\tdef __init__(self, Agent : BuffedDQNAgent) -> None:\n",
    "\t\tself.training_agent = Agent(1)\n",
    "\t\tself.env_1 = Environnement(1)\n",
    "\n",
    "\t\tself.to_beat_agent = Agent(2)\n",
    "\t\tself.env_2 = Environnement(2)\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t# Player 1\n",
    "\t\tself.env_1.reset()\n",
    "\t\tstate_1 = self.env_1.get_state()\n",
    "\n",
    "\t\t# Player 2\n",
    "\t\tself.env_2.reset()\n",
    "\t\tstate_2 = self.env_2.get_state()\n",
    "\t\treturn state_1, state_2\n",
    "\t\t\n",
    "\tdef compute_reward(self, old_state, next_state):\n",
    "\t\t# TODO\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef train(self, steps=10, log_each=1, skip_frames=4, reset_agent_each=10):\n",
    "\t\tstate_1, state_2 = self.reset()\n",
    "\t\tcurrent_skip = skip_frames\n",
    "\t\t\n",
    "\t\ti = 1\n",
    "\n",
    "\t\twhile i <= steps:\n",
    "\t\t\tgame_over = False\n",
    "\n",
    "\t\t\tdata_point = {\"obs\": state_2} # need [obs, action, reward, next_obs, done]\n",
    "\n",
    "\t\t\twhile current_skip > 0 and not game_over:\n",
    "\t\t\t\twith torch.no_grad(): # Don't compute grad for game playing\n",
    "\t\t\t\t\t# To_beat_agent turn\n",
    "\t\t\t\t\ttobeat_action = self.to_beat_agent.get_action(state_1)\n",
    "\t\t\t\t\tstate_1 = self.env_1.do_action(tobeat_action)\n",
    "\t\t\t\t\t# Then the training agent turn\n",
    "\t\t\t\t\ttraining_action = self.training_agent.get_action(state_2)\n",
    "\t\t\t\t\tstate_2 = self.env_2.do_action(training_action)\n",
    "\n",
    "\t\t\t\t\t# if it is the first step of the observation\n",
    "\t\t\t\t\tif current_skip == skip_frames:\n",
    "\t\t\t\t\t\tdata_point[\"action\"] = training_action \n",
    "\n",
    "\t\t\t\t\tcurrent_skip -= 1\n",
    "\n",
    "\t\t\t\t\tif (state_2.winner is not None):\n",
    "\t\t\t\t\t\tgame_over = True\n",
    "\t\t\t\n",
    "\t\t\tdata_point[\"next_obs\"] = state_2\n",
    "\t\t\tdata_point[\"reward\"] = self.compute_reward(data_point[\"obs\"], data_point[\"next_obs\"])\n",
    "\t\t\tdata_point[\"done\"] = game_over\n",
    "\n",
    "\t\t\tif game_over:\n",
    "\t\t\t\tstate_1, state_2 = self.reset()\n",
    "\n",
    "\t\t\t# TODO : add to replay buffer and train with replay buffer\n",
    "\n",
    "\t\t\tif i % reset_agent_each == 0: # reset the to beat agent to the current trained agent\n",
    "\t\t\t\tself.to_beat_agent.brain.load_state_dict(self.training_agent.brain.state_dict())\n",
    "\n",
    "\t\t\ti += 1 # TODO : only update i when a step of training is done with the memory\n",
    "\t\t\tprint(f\"{steps} steps left\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3336df672a544e285a8d6fe4ff0ef23e5aa65345aa81e9136b09e43bc523f0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ReinforcmentLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
