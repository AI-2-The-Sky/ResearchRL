{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "## a reinforcement learning implementation\n",
    "\n",
    "This little program is an implementation of a basic AI that trains in order to resolves the Gym's FrozenLake environment.\n",
    "It is solely based on the Q-Learning algorithm, and so without any NN features.\n",
    "But it can be really helpful to understand the basics of the more advanced DQN algorithm and the limits of the classic Q-Learning.\n",
    "\n",
    "It was made by following Maxime Labonne's article ([Q-Learning for beginners](https://mlabonne.github.io/blog/reinforcement%20learning/q-learning/frozen%20lake/gym/tutorial/2022/02/13/Q_learning.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frozen Lake Environment\n",
    "\n",
    "The frozen lake by default of a 4x4 grid on which there is 3 types of tiles:\n",
    "- Neutral tiles (ice), on which the agent can be without any consequences\n",
    "- Death tiles (holes), on which the agent dies and must then restart from the beginning\n",
    "- Win tiles (present), on which the agent gain a reward and then restart from the beginning\n",
    "\n",
    "Futhermore, because every action in the environment is uncertain (the ground is slippery).  \n",
    "There is constantly a 33% chance that agent do an action that is not intended (slip to another tile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-Table\n",
    "\n",
    "The agent learns in the environment by registers the value of every actions in every states.\n",
    "In the FrozenLake there is 4 actions for 16 states and so a Q-Table of 64 value to discover while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_actions = env.action_space.n\n",
    "nb_states = env.observation_space.n\n",
    "qtable = np.zeros((nb_states, nb_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The RL model have different takes multiples **hyperparameters** that define the agent learning's behavior.\n",
    "\n",
    "- episodes -> simply the number of try that will do the agent to gain rewards\n",
    "- alpha -> the learning rate\n",
    "- gamma -> the discount factor\n",
    "- espilon -> the agent curiosity (Exploration vs Exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1000\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists that will stores every episodes outcomes to let us do somes fancy graph with matplotlib\n",
    "outcomes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "The agent discover/define every value of the qtable by backpropagate the value of each tiles\n",
    "After each actions the agent backpropagate the value of the current maximum (action - state) to the previous action state (The value is indeed modified by the hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    outcomes.append(\"Failure\")\n",
    "\n",
    "    while not done:\n",
    "        rnd = np.random.random()\n",
    "\n",
    "        if rnd < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(qtable[state])\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        qtable[state, action] = qtable[state, action] + alpha * (reward + gamma * np.max(qtable[new_state]) - qtable[state, action])\n",
    "        state = new_state\n",
    "\n",
    "        if reward:\n",
    "            outcomes[-1] = \"Success\"\n",
    "    epsilon = max(epsilon - epsilon_decay, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw outcomes fancy graph (in blue when successful)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.xlabel(\"Run number\")\n",
    "plt.ylabel(\"Outcome\")\n",
    "plt.bar(range(len(outcomes)), outcomes, color=\"#0A047A\", width=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCL:\n",
    "\n",
    "The Q-Learning algorithm works pretty well on small and simple environment like the FrozenLake but meets its limits in much complex or huge environment (more actions, more state) because the qtable must follow the multiplication of those two factors.  \n",
    "And our poors computers can't handle a 10000 x 10000 qtable that easily.  \n",
    "For better performance **we need approximation**.  \n",
    "An approximation that gives us **NN**."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
