{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf0d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import gym.envs.toy_text.frozen_lake as frozen_lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682d39a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_4x4_map =[ \"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\" ]\n",
    "default_8x8_map = [ \"SFFFFFFF\", \"FFFFFFFF\", \"FFFHFFFF\", \"FFFFFHFF\", \"FFFHFFFF\", \"FHHFFFHF\", \"FHFFHFHF\", \"FFFHFFFG\" ]\n",
    "\n",
    "def random_4x4_lake_map():\n",
    "    return frozen_lake.generate_random_map(size=4, p=torch.rand(1).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc512e92",
   "metadata": {},
   "source": [
    "# First environment\n",
    "\n",
    "Lets first code an agent with random behaviour to grasp gym 'FrozenLake' environment's api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c37ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gym.openai.com/docs/\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "for _ in range(300):\n",
    "    env.render()\n",
    "    done = env.step(env.action_space.sample()) # take a random action\n",
    "    if done : break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d94f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "is_slippery = True\n",
    "lake_map = default_4x4_map\n",
    "render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5745cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 3 timesteps and got 0.0 reward\n",
      "Episode 2 finished after 6 timesteps and got 0.0 reward\n",
      "Episode 3 finished after 5 timesteps and got 0.0 reward\n",
      "Episode 4 finished after 16 timesteps and got 1.0 reward\n",
      "Episode 5 finished after 7 timesteps and got 0.0 reward\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=lake_map, is_slippery=is_slippery)\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    if render : env.render()\n",
    "    for t in range(100):\n",
    "        action = env.action_space.sample() # random action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if render : env.render()\n",
    "        if done : break\n",
    "    print(\"Episode {} finished after {} timesteps and got {} reward\".format(i_episode+1, t+1, reward))        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9439a",
   "metadata": {},
   "source": [
    "# Value update / Bellman update\n",
    "\n",
    "Based on [the first lecture of the deep RL bootcamp](https://www.youtube.com/watch?v=qaMdN6LS9rA) I wanna find the cost of every tile of the map\n",
    "\n",
    "\n",
    "### Compute qval grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45fe58c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my FrozenLake implementation of the bellman update\n",
    "def fl_bellman_update(lake_map, is_slippery, discount, iterations) :\n",
    "    shape = (len(lake_map), len(lake_map[0]), 4)\n",
    "    lake_reward_map = torch.zeros(shape)\n",
    "    exit_coord = []\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            if lake_map[i][j] == 'H' :\n",
    "                lake_reward_map[i, j, :] = -1\n",
    "                exit_coord.append((i,j))\n",
    "            if lake_map[i][j] == 'G' :\n",
    "                lake_reward_map[i, j, :] = 1\n",
    "                exit_coord.append((i,j))\n",
    "\n",
    "    for k in range(iterations):\n",
    "        prev_lake_reward_map = lake_reward_map.clone()\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                if not any((i,j) == coord for coord in exit_coord):\n",
    "\n",
    "                    left = prev_lake_reward_map[i, [j-1 if j-1>=0 else j], :].max()\n",
    "                    down = prev_lake_reward_map[[i+1 if i+1<shape[0] else i], j, :].max()\n",
    "                    right = prev_lake_reward_map[i, [j+1 if j+1<shape[1] else j], :].max()\n",
    "                    up = prev_lake_reward_map[[i-1 if i-1>=0 else i], j, :].max()\n",
    "\n",
    "                    if is_slippery :\n",
    "                        lake_reward_map[i, j, 0] = discount * (left + down + up) / 3\n",
    "                        lake_reward_map[i, j, 1] = discount * (down + left + right) / 3 \n",
    "                        lake_reward_map[i, j, 2] = discount * (right + down + up) / 3\n",
    "                        lake_reward_map[i, j, 3] = discount * (up + left + right) / 3\n",
    "\n",
    "                    else :\n",
    "                        lake_reward_map[i, j, 0] = discount * left\n",
    "                        lake_reward_map[i, j, 1] = discount * down\n",
    "                        lake_reward_map[i, j, 2] = discount * right\n",
    "                        lake_reward_map[i, j, 3] = discount * up\n",
    "                        \n",
    "    return lake_reward_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e7de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1301,  0.1186,  0.1186,  0.1121],\n",
      "         [-0.2458, -0.2525, -0.2640,  0.0938],\n",
      "         [-0.0178, -0.0209, -0.0277,  0.0724],\n",
      "         [-0.2739, -0.2739, -0.2771,  0.0625]],\n",
      "\n",
      "        [[ 0.1507, -0.2072, -0.2137, -0.2278],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-0.2223, -0.5619, -0.2223, -0.6104],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-0.2072, -0.1693, -0.1834,  0.1951],\n",
      "         [-0.1178,  0.2703, -0.1082, -0.1835],\n",
      "         [ 0.2255, -0.0208, -0.1768, -0.3015],\n",
      "         [-1.0000, -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000, -1.0000],\n",
      "         [-0.0940,  0.0307,  0.4330, -0.0208],\n",
      "         [ 0.4188,  0.6641,  0.5983,  0.5252],\n",
      "         [ 1.0000,  1.0000,  1.0000,  1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "is_slippery = True\n",
    "discount = 0.95\n",
    "lake_map = default_4x4_map\n",
    "\n",
    "lake_reward_map = fl_bellman_update(lake_map, is_slippery, discount, iterations)\n",
    "print(lake_reward_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277f56e",
   "metadata": {},
   "source": [
    "### Make simple agent folowing max qval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5679318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl_get_action(lake_reward_map, observation):\n",
    "    row_len = lake_reward_map.size()[0]\n",
    "    x = math.trunc(observation / row_len)\n",
    "    y = observation % row_len\n",
    "    \n",
    "    return lake_reward_map[x, y, :].argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c08ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 100 timesteps and got 0.0 reward\n",
      "Episode 2 finished after 29 timesteps and got 1.0 reward\n",
      "Episode 3 finished after 60 timesteps and got 1.0 reward\n",
      "Episode 4 finished after 62 timesteps and got 1.0 reward\n",
      "Episode 5 finished after 100 timesteps and got 0.0 reward\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=lake_map, is_slippery=is_slippery)\n",
    "render = True\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    if render : env.render()\n",
    "    for t in range(100):\n",
    "        action = fl_get_action(lake_reward_map, observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if render : env.render()\n",
    "        if done : break\n",
    "    print(\"Episode {} finished after {} timesteps and got {} reward\".format(i_episode+1, t+1, reward))        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854ee41",
   "metadata": {},
   "source": [
    "### Move agent inside a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bee27a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, is_slippery:bool=True, lake_map:torch.tensor=default_4x4_map, discount:float=0.95, iterations:int=None) -> None:\n",
    "        shape = (len(lake_map), len(lake_map[0]))\n",
    "        area = shape[0]*shape[1]\n",
    "        if iterations == None :\n",
    "            iterations = area * 5\n",
    "            \n",
    "        self.lake_reward_map = fl_bellman_update(lake_map, is_slippery, discount, iterations)     \n",
    "\n",
    "    def get_action(self, state: int):\n",
    "        return fl_get_action(self.lake_reward_map, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15916005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished after 50 timesteps and got 1.0 reward\n",
      "Episode 2 finished after 43 timesteps and got 1.0 reward\n",
      "Episode 3 finished after 41 timesteps and got 1.0 reward\n",
      "Episode 4 finished after 88 timesteps and got 1.0 reward\n",
      "Episode 5 finished after 100 timesteps and got 0.0 reward\n"
     ]
    }
   ],
   "source": [
    "render = True\n",
    "lake_map = default_8x8_map\n",
    "is_slippery = True\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=lake_map, is_slippery=is_slippery)\n",
    "agent = Agent(is_slippery=is_slippery, lake_map=lake_map)\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    if render : env.render()\n",
    "    for t in range(1000):\n",
    "        action = agent.get_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if render : env.render()\n",
    "        if done : break\n",
    "    print(\"Episode {} finished after {} timesteps and got {} reward\".format(i_episode+1, t+1, reward))        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251462c",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Now that we've implemented the iterative method to get the Q-value we're gonna implement the exploratory method because the iterative method has one huge downside : it evaluates each and every possible move without any optimisation whereas the q-learning algorithm give us the possibility to explore in a smarter, more efficient manner.\n",
    "\n",
    "Here are some of my references: [toward data science](https://towardsdatascience.com/q-learning-algorithm-from-explanation-to-implementation-cdbeda2ea187), [playing Atari with DRL](https://arxiv.org/pdf/1312.5602.pdf), [second lecture of the RL bootcamp](https://discord.com/channels/@me/966660137215467560/967833369478053988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a478e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller learning_rate_decay_factor means faster decay\n",
    "# epsylon: 0 => never explore, 1 => random exploration\n",
    "def fl_qlearn(lake_map, is_slippery, discount, learning_time_steps, learning_rate_decay_factor:float=0, epsylon:float=0.001) :\n",
    "    shape = (len(lake_map), len(lake_map[0]), 4)\n",
    "    lake_reward_map = torch.zeros(shape)\n",
    "    exit_coord = []\n",
    "    start_coord = (0,0)\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            if lake_map[i][j] == 'H' :\n",
    "                lake_reward_map[i, j, :] = -1\n",
    "                exit_coord.append((i,j))\n",
    "            if lake_map[i][j] == 'G' :\n",
    "                lake_reward_map[i, j, :] = 1\n",
    "                exit_coord.append((i,j))\n",
    "            if lake_map[i][j] == 'S' :\n",
    "                start_coord = (i,j)\n",
    "                \n",
    "    runner_coord = start_coord\n",
    "    for t in range(learning_time_steps):\n",
    "        action = epsylon_greedy(lake_reward_map[runner_coord[0], runner_coord[1]], epsylon)\n",
    "        new_runner_coord = move(runner_coord, action, shape, is_slippery)\n",
    "        \n",
    "        learning_rate = math.e ** (-t*learning_rate_decay_factor/learning_time_steps)\n",
    "        lake_reward_map[runner_coord[0], runner_coord[1], action] = (\n",
    "            lake_reward_map[runner_coord[0], runner_coord[1], action]\n",
    "            + (learning_rate\n",
    "               * (discount * lake_reward_map[new_runner_coord[0], new_runner_coord[1], :].max()\n",
    "                  - lake_reward_map[runner_coord[0], runner_coord[1], action]\n",
    "                 )\n",
    "              )\n",
    "        )\n",
    "        \n",
    "        if any(new_runner_coord == coord for coord in exit_coord) :\n",
    "            runner_coord = start_coord\n",
    "        else :\n",
    "            runner_coord = new_runner_coord\n",
    "            \n",
    "    return lake_reward_map         \n",
    "    \n",
    "def move(coord, action, shape, is_slippery):\n",
    "    action_before_noise = action\n",
    "    if is_slippery :\n",
    "        noise = math.trunc(torch.rand(1).item() * 3)\n",
    "        if noise == 0 :\n",
    "            action = (3 if action-1<0 else action-1)\n",
    "        elif noise == 1 :\n",
    "            action = (0 if action+1>3 else action+1)\n",
    "        \n",
    "    i, j = coord[0], coord[1]\n",
    "    if action == 0 :\n",
    "        j = (j-1 if j-1>=0 else j)\n",
    "    elif action == 1 :\n",
    "        i = i+1 if i+1<shape[0] else i\n",
    "    elif action == 2 :\n",
    "        j = j+1 if j+1<shape[1] else j\n",
    "    else :\n",
    "        i = i-1 if i-1>=0 else i\n",
    "        \n",
    "    return i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9ea04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration actor strategies\n",
    "\n",
    "def rand_action():\n",
    "    return math.trunc(torch.rand(1).item() * 4)\n",
    "    \n",
    "def optimal_strategy(actions_reward_map):\n",
    "    return actions_reward_map.argmax().item()\n",
    "\n",
    "def epsylon_greedy(actions_reward_map, epsylon):\n",
    "    if actions_reward_map.max() <= 0 or torch.rand(1).item() < epsylon :\n",
    "        return rand_action()\n",
    "    else :\n",
    "        return optimal_strategy(actions_reward_map)\n",
    "\n",
    "# The highest the curently evaluated future reward, the more likely it is to folow the strategy\n",
    "def heat_seeking(actions_reward_map, epsylon):\n",
    "    heat_map = (math.sqrt(1/epsylon) ** actions_reward_map)\n",
    "    heat = math.trunc(torch.rand(1).item() * heat_map.sum().item())\n",
    "    cumulative_heat = 0\n",
    "    for i,h in enumerate(heat_map) :\n",
    "        cumulative_heat += h\n",
    "        if heat <= cumulative_heat :\n",
    "            return i\n",
    "        \n",
    "def ibrid_strategy(actions_reward_map, epsylon):\n",
    "    # should be better tuned\n",
    "    if actions_reward_map.max() <= 0 or torch.rand(1).item() < epsylon :\n",
    "        return heat_seeking(actions_reward_map, epsylon)\n",
    "    else :\n",
    "        return optimal_strategy(actions_reward_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91907a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, is_slippery:bool=True, lake_map:torch.tensor=default_4x4_map, discount:float=0.95, learning_time_steps:int=None, learning_rate:float=0.3) -> None:\n",
    "        shape = (len(lake_map), len(lake_map[0]))\n",
    "        area = shape[0]*shape[1]\n",
    "        if learning_time_steps == None :\n",
    "            learning_time_steps = area * 10000\n",
    "            \n",
    "        self.lake_reward_map = fl_qlearn(lake_map, is_slippery, discount, learning_time_steps, learning_rate) \n",
    "        print(self.lake_reward_map)\n",
    "\n",
    "    def get_action(self, state: int):\n",
    "        return fl_get_action(self.lake_reward_map, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79951ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7516e-04, -4.7012e-09, -1.7402e-04,  1.2612e-44],\n",
      "         [-3.6753e-02, -1.1112e-03, -2.3763e-01,  1.2612e-44],\n",
      "         [-6.6676e-03, -2.5718e-04, -1.8007e-05,  1.2612e-44],\n",
      "         [-1.3068e-03, -1.2220e-03, -9.4600e-01,  1.2612e-44]],\n",
      "\n",
      "        [[-2.4530e-04, -1.5336e-01, -1.9697e-03, -3.0712e-02],\n",
      "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
      "         [-1.9499e-03, -9.2210e-01, -9.4995e-01, -9.4904e-01],\n",
      "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00]],\n",
      "\n",
      "        [[-7.7792e-03, -8.8595e-01, -9.4248e-01,  3.1902e-02],\n",
      "         [-9.4773e-01, -3.7809e-04, -9.3722e-01, -6.9804e-01],\n",
      "         [-1.2293e-03,  6.7195e-01, -9.4298e-01, -1.4618e-03],\n",
      "         [-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00]],\n",
      "\n",
      "        [[-1.0000e+00, -1.0000e+00, -1.0000e+00, -1.0000e+00],\n",
      "         [ 4.9487e-03,  0.0000e+00,  6.8145e-01,  0.0000e+00],\n",
      "         [ 0.0000e+00,  7.7011e-01,  0.0000e+00,  0.0000e+00],\n",
      "         [ 1.0000e+00,  1.0000e+00,  1.0000e+00,  1.0000e+00]]])\n",
      "Episode 1 finished after 100 timesteps and got 0.0 reward\n",
      "Episode 2 finished after 100 timesteps and got 0.0 reward\n",
      "Episode 3 finished after 100 timesteps and got 0.0 reward\n",
      "Episode 4 finished after 100 timesteps and got 0.0 reward\n",
      "Episode 5 finished after 100 timesteps and got 0.0 reward\n"
     ]
    }
   ],
   "source": [
    "render = True\n",
    "lake_map = default_4x4_map\n",
    "is_slippery = True\n",
    "\n",
    "env = gym.make('FrozenLake-v1', desc=lake_map, is_slippery=is_slippery)\n",
    "agent = Agent(is_slippery=is_slippery, lake_map=lake_map)\n",
    "\n",
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    if render : env.render()\n",
    "    for t in range(100):\n",
    "        action = agent.get_action(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if render : env.render()\n",
    "        if done : break\n",
    "    print(\"Episode {} finished after {} timesteps and got {} reward\".format(i_episode+1, t+1, reward))        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5696c",
   "metadata": {},
   "source": [
    "Its not converging, something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c63b132",
   "metadata": {},
   "source": [
    "## Second try\n",
    "\n",
    "I had some issues with my previous implementation of the q-learning algorithm. When looking at other peoples implementations I realized that mine could be significantly improved in terms of readability and  structure. So I set out to make an other version.\n",
    "\n",
    "This code is pritty inspired by Juliette's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef0f3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward after 0 training runs : 0.0\n",
      "Average reward after 1000 training runs : 0.063\n",
      "Average reward after 2000 training runs : 0.091\n",
      "Average reward after 3000 training runs : 0.428\n",
      "Average reward after 4000 training runs : 0.488\n",
      "Average reward after 5000 training runs : 0.413\n",
      "Average reward after 6000 training runs : 0.492\n",
      "Average reward after 7000 training runs : 0.425\n",
      "Average reward after 8000 training runs : 0.39\n",
      "Average reward after 9000 training runs : 0.477\n",
      "Average reward after 10000 training runs : 0.457\n",
      "Average reward after 1000 runs with optimal strategy : 0.421\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "is_slippery = True\n",
    "lake_map = default_4x4_map\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", desc=lake_map, is_slippery=is_slippery)\n",
    "env.reset()\n",
    "#env.render()\n",
    "\n",
    "discount = 0.9\n",
    "rewards = 0\n",
    "thousand_runs = 10\n",
    "qtable = torch.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "for t in range(thousand_runs * 1000 + 1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    if t % 1000 == 0 :\n",
    "        print('Average reward after {} training runs : {}'.format(t, rewards/1000))\n",
    "        #print(qtable)\n",
    "        #print('\\n------------------------------------------------------------------\\n')\n",
    "        rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsylon_greedy(qtable[state], 0.005)\n",
    "        #action = heat_seeking(qtable[state], 0.000000000001)\n",
    "        learning_rate = math.e ** (-t/10000)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + discount * torch.max(qtable[new_state]) - qtable[state, action])\n",
    "        state = new_state\n",
    "        rewards += reward\n",
    "\n",
    "        #env.render()\n",
    "        \n",
    "rewards = 0\n",
    "for t in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = optimal_strategy(qtable[state])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        rewards += reward\n",
    "\n",
    "        #env.render()\n",
    "        \n",
    "print('Average reward after 1000 runs with optimal strategy : {}'.format(rewards/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9229b1",
   "metadata": {},
   "source": [
    "The heat-seeking training policy that I implemented seem less promissing than the classic epsylon-greedy strategy because it is harder to tune the exploration vs optimal policy dilema. Though I think that that type of heat-seeking algorithm could help explore in a smarter way than epsylon-greedy but I bellive the maximum improvement to be limited.\n",
    "\n",
    "\n",
    "## Experience replay\n",
    "\n",
    "In [this paper](https://arxiv.org/pdf/1312.5602.pdf) (section 4.0) they describe a technique called *experience replay*. I would like to try implementing it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0efcc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward after 0 training runs : 0.0\n",
      "Average reward after 1000 training runs : 0.028\n",
      "Average reward after 2000 training runs : 0.031\n",
      "Average reward after 3000 training runs : 0.179\n",
      "Average reward after 4000 training runs : 0.301\n",
      "Average reward after 5000 training runs : 0.518\n",
      "Average reward after 6000 training runs : 0.488\n",
      "Average reward after 7000 training runs : 0.567\n",
      "Average reward after 8000 training runs : 0.581\n",
      "Average reward after 9000 training runs : 0.543\n",
      "Average reward after 10000 training runs : 0.542\n",
      "Average reward after 1000 runs with optimal strategy : 0.628\n"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "is_slippery = True\n",
    "lake_map = default_4x4_map\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", desc=lake_map, is_slippery=is_slippery)\n",
    "env.reset()\n",
    "\n",
    "discount = 0.9\n",
    "epsylon = 0.001\n",
    "rewards = 0\n",
    "thousand_runs = 10\n",
    "replay_memory = []\n",
    "replay_memory_max_size = 2000\n",
    "qtable = torch.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "for t in range(thousand_runs * 1000 + 1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    if t % 1000 == 0 :\n",
    "        print('Average reward after {} training runs : {}'.format(t, rewards/1000))\n",
    "        #print(qtable)\n",
    "        #print('\\n------------------------------------------------------------------\\n')\n",
    "        rewards = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsylon_greedy(qtable[state], epsylon)\n",
    "        learning_rate = math.e ** (-t/10000)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        optimal_next_step = torch.max(qtable[new_state])\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + discount * optimal_next_step - qtable[state, action])\n",
    "\n",
    "        # if the step teaches something to the model then add it to replay memory\n",
    "        if abs(reward + discount * optimal_next_step) > 0.1 :\n",
    "            replay_memory.append((state,action,reward,new_state))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards += reward\n",
    "        \n",
    "    replay_memory_size = len(replay_memory)\n",
    "    \n",
    "    # we keep the memory size at max popping the last added elements\n",
    "    d = replay_memory_size - replay_memory_max_size\n",
    "    while d > 0 :\n",
    "        replay_memory.pop(0)\n",
    "        d -= 1\n",
    "            \n",
    "    # we pick random replays and train the model\n",
    "    minibatch_size = math.trunc(replay_memory_size/20)\n",
    "    random_indices = map(math.trunc, torch.rand(minibatch_size).tolist() * replay_memory_max_size)\n",
    "    replays = [v for i, v in enumerate(replay_memory) if i in random_indices]\n",
    "    for _, (s,a,r,ns) in enumerate(replays) :\n",
    "        qtable[s, a] = qtable[s, a] + learning_rate * (r + discount * torch.max(qtable[ns]) - qtable[s, a])\n",
    "\n",
    "        \n",
    "rewards = 0\n",
    "for t in range(1000):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = optimal_strategy(qtable[state])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        state = new_state\n",
    "        rewards += reward\n",
    "        \n",
    "print('Average reward after 1000 runs with optimal strategy : {}'.format(rewards/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f710d",
   "metadata": {},
   "source": [
    "Afterwards I realised that I misunderstood experiance-replay. It isn't used to be more efficient in discovering the Q-Table but to train a DNN that will aproximate the function that finds the q-value. The huge benefit is that we will be able to generalize this DNN even for any action space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b4b5",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "Now that I get how Q-learning works lets get into DQN. My main resource here is [this toward data science article](https://towardsdatascience.com/deep-q-networks-theory-and-implementation-37543f60dd67)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4569f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for 4x4 grids\n",
    "class ModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelV0, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "             nn.Linear(17, 128),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(128, 32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32, 4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, lake_tensor): \n",
    "        x = torch.cat((torch.as_tensor([state]), lake_tensor))\n",
    "        x = self.cnn(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc37660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lake_map_to_tensor(lake_map):\n",
    "    lake_map = ''.join(lake_map)\n",
    "    mytensor = torch.zeros(len(lake_map), requires_grad=False)\n",
    "    for i,v in enumerate(lake_map):\n",
    "        if v == 'H' :\n",
    "            mytensor[i] = -1\n",
    "        if v == 'G' :\n",
    "            mytensor[i] = 1\n",
    "    return mytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "001b4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward after 0 training runs : 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(thousand_runs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 18\u001b[0m     lake_map \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_4x4_lake_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrozenLake-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, desc\u001b[38;5;241m=\u001b[39mlake_map, is_slippery\u001b[38;5;241m=\u001b[39mis_slippery)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mrandom_4x4_lake_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_4x4_lake_map\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrozen_lake\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_random_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/gym/envs/toy_text/frozen_lake.py:61\u001b[0m, in \u001b[0;36mgenerate_random_map\u001b[0;34m(size, p)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid:\n\u001b[1;32m     60\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1\u001b[39m, p)\n\u001b[0;32m---> 61\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     res[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     res[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32mmtrand.pyx:901\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "is_slippery = True\n",
    "discount = 0.9\n",
    "epsylon = 0.01\n",
    "thousand_runs = 10\n",
    "replay_memory = []\n",
    "replay_memory_max_size = 2000\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ModelV0().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "### TRAINING ###\n",
    "\n",
    "rewards = 0\n",
    "for t in range(thousand_runs * 1000 + 1):\n",
    "    lake_map = random_4x4_lake_map()\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=lake_map, is_slippery=is_slippery)\n",
    "\n",
    "    if t % 1000 == 0 :\n",
    "        print('Average reward after {} training runs : {}'.format(t, rewards/1000))\n",
    "        rewards = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    map_tensor = lake_map_to_tensor(lake_map)\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions_weights = model(state, map_tensor)\n",
    "        action = rand_action()\n",
    "        if not (actions_weights.max() <= 0 or torch.rand(1).item() < epsylon) :\n",
    "            action = actions_weights.argmax().item()   \n",
    "        action_weight = actions_weights[action].item()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        next_step_best_action_weight = reward + discount * model(new_state, map_tensor).max()\n",
    "        \n",
    "        if abs(discount * next_step_best_action_weight + reward) > 0.1 :\n",
    "            replay_memory.append((action,actions_weights,next_step_best_action_weight))\n",
    "\n",
    "        state = new_state\n",
    "        rewards += reward\n",
    "\n",
    "        \n",
    "    replay_memory_size = len(replay_memory)\n",
    "\n",
    "    # we keep the memory size at max popping the last added elements\n",
    "    d = replay_memory_size - replay_memory_max_size\n",
    "    while d > 0 :\n",
    "        replay_memory.pop(0)\n",
    "        d -= 1\n",
    "\n",
    "    # we pick random replays and train the model\n",
    "    learning_rate = math.e ** (-t/(thousand_runs * 1000))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    minibatch_size = math.trunc(replay_memory_size/20)\n",
    "    random_indices = map(math.trunc, torch.rand(minibatch_size).tolist() * replay_memory_max_size)\n",
    "    replays = [v for i, v in enumerate(replay_memory) if i in random_indices]\n",
    "    for _, (a,aw,nv) in enumerate(replays) :\n",
    "        naw = aw.clone()\n",
    "        naw[a] = nv\n",
    "        loss = loss_fn(aw, naw)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "                                \n",
    "### TESTING ###\n",
    "        \n",
    "rewards = 0\n",
    "for t in range(1000):\n",
    "    lake_map = random_4x4_lake_map()\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=lake_map, is_slippery=is_slippery)\n",
    "    \n",
    "    map_tensor = lake_map_to_tensor(lake_map)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model(state, lake_map)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        state = new_state\n",
    "        rewards += reward\n",
    "        \n",
    "                                \n",
    "env.close()\n",
    "print('Average reward after 1000 runs with optimal strategy : {}'.format(rewards/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96a2c5",
   "metadata": {},
   "source": [
    "## Rework\n",
    "\n",
    "We reviewed the code above with [ezalos](https://github.com/ezalos) and he made it significantly better by moving everithing into a trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f498e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_trainer import MyTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c828ab",
   "metadata": {},
   "source": [
    "Now we can use it to itterate our model while saving the results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1076a324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward after 100 training runs : 0.03\n",
      "Average reward after 200 training runs : 0.04\n",
      "Average reward after 300 training runs : 0.05\n",
      "Average reward after 400 training runs : 0.02\n",
      "Average reward after 500 training runs : 0.05\n",
      "Average reward after 600 training runs : 0.0\n",
      "Average reward after 700 training runs : 0.03\n",
      "Average reward after 800 training runs : 0.0\n",
      "Average reward after 900 training runs : 0.03\n",
      "Average reward after 1000 training runs : 0.03\n",
      "Average reward after 1000 runs with optimal strategy : 0.0\n"
     ]
    }
   ],
   "source": [
    "class Model00(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model00, self).__init__()\n",
    "        self.dense = nn.Sequential(\n",
    "             nn.Linear(17, 128),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(128, 32),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(32, 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, lake_tensor):\n",
    "        x = torch.cat((torch.as_tensor([state]), lake_tensor))\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "model = Model00()\n",
    "param_dict = {\n",
    "    \"is_slippery\" : True,\n",
    "    \"randomize_lake_map\" : False,\n",
    "    \"discount\" : 0.9,\n",
    "    \"epsylon\" : 0.01,\n",
    "    \"hundred_runs\" : 10,\n",
    "    \"replay_memory_max_size\" : 2000,\n",
    "    \"replay_regularity\" : 20,\n",
    "    \"model\" : model,\n",
    "    \"loss_fn\" : nn.MSELoss(),\n",
    "    \"minibatch_size\" : 32,\n",
    "    \"output_folder\" : model.__class__.__name__\n",
    "}\n",
    "MyTrainer(**param_dict).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
