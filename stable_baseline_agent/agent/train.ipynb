{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bomberman.Environment import Environnement\n",
    "from bomberman.states.State import State\n",
    "import torch\n",
    "from DQN_Agent import BuffedDQNAgent\n",
    "import time\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer without batch.\"\"\"\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        self.buffer = []\n",
    "\n",
    "        self.max_size = size\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        data_point : Dict[str, State or List[int] or float or bool],\n",
    "    ):\n",
    "        self.buffer.append(data_point)\n",
    "        if len(self.buffer) > self.max_size:\n",
    "            self.buffer.pop(-1)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, State or List[int] or float or bool]:\n",
    "        return self.buffer[np.random.choice(len(self))]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\tdef __init__(\n",
    "\t\tself, \n",
    "\t\tAgent : BuffedDQNAgent, \n",
    "        memory_size: int=10000,\n",
    "        target_update: int=100,\n",
    "        epsilon_decay: float = 1e-3,\n",
    "        max_epsilon: float = 0.2,\n",
    "        min_epsilon: float = 0.001,\n",
    "        gamma: float = 0.99\n",
    "\t) -> None:\n",
    "\t\tself.training_agent = Agent(1)\n",
    "\t\tself.env_training = Environnement(1)\n",
    "\n",
    "\t\tself.to_beat_agent = Agent(2)\n",
    "\t\tself.env_to_beat = Environnement(2)\n",
    "\n",
    "\t\tself.memory = ReplayBuffer(memory_size)\n",
    "\n",
    "\t\tself.epsilon = max_epsilon\n",
    "\t\tself.epsilon_decay = epsilon_decay\n",
    "\t\tself.min_epsilon = min_epsilon\n",
    "\t\tself.max_epsilon = max_epsilon\n",
    "\n",
    "\t\tself.target_update = target_update\n",
    "\n",
    "\t\tself.gamma = gamma\n",
    "\n",
    "\t\tself.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t# Player 1\n",
    "\t\tstate_training = self.env_training.reset()\n",
    "\n",
    "\t\t# Player 2\n",
    "\t\tstate_to_beat = self.env_to_beat.get_state()\n",
    "\t\treturn state_training, state_to_beat\n",
    "\t\t\n",
    "\tdef compute_reward(self, old_state, next_state, game_over):\n",
    "\t\twinner = next_state.as_tuple()[2]\n",
    "\t\t\n",
    "\t\tif game_over: # if game is over and player is not the winner, return -1 else 1\n",
    "\t\t\treturn -10. if not winner or winner != self.training_agent.player_num else 10.\n",
    "\t\t\n",
    "\t\treturn -1\n",
    "\t\n",
    "\tdef _plot(\n",
    "\t\tself, \n",
    "\t\tframe_idx: int, \n",
    "\t\tscores: List[float], \n",
    "\t\tlosses: List[float], \n",
    "\t\tepsilons: List[float],\n",
    "\t):\n",
    "\t\t\"\"\"Plot the training progresses.\"\"\"\n",
    "\t\tclear_output(True)\n",
    "\t\tplt.figure(figsize=(20, 5))\n",
    "\t\tplt.subplot(131)\n",
    "\t\tplt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "\t\tplt.plot(scores)\n",
    "\t\tplt.subplot(132)\n",
    "\t\tplt.title('loss')\n",
    "\t\tplt.plot(losses)\n",
    "\t\tplt.subplot(133)\n",
    "\t\tplt.title('epsilons')\n",
    "\t\tplt.plot(epsilons)\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef train(self, steps=10, log_each=1, skip_frames=4, reset_agent_each=10, max_duration=50):\n",
    "\t\tstate_training, state_to_beat = self.reset()\n",
    "\t\tcurrent_skip = skip_frames\n",
    "\t\t\n",
    "\t\ti = 1\n",
    "\n",
    "\t\t# Use a game duration to force agent to finish the game fast\n",
    "\t\t# (and avoid starting agent which are random to never end playing)\n",
    "\t\tgame_duration = 0\n",
    "\n",
    "\t\tepsilons = []\n",
    "\t\tlosses = []\n",
    "\t\tscores = []\n",
    "\t\tscore = 0\n",
    "\t\tupdate_count = 0\n",
    "\n",
    "\t\twhile i <= steps:\n",
    "\t\t\tgame_over = False\n",
    "\n",
    "\t\t\tdata_point = {\"obs\": state_training} # need [obs, action, reward, next_obs, done]\n",
    "\n",
    "\t\t\twhile current_skip > 0 and not game_over:\n",
    "\t\t\t\twith torch.no_grad(): # Don't compute grad for game playing\n",
    "\t\t\t\t\ttime.sleep(0.001)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# To_beat_agent turn\n",
    "\t\t\t\t\ttobeat_action = self.to_beat_agent.get_action(state_to_beat)\n",
    "\t\t\t\t\tstate_to_beat = self.env_to_beat.do_action(tobeat_action)\n",
    "\n",
    "\t\t\t\t\t# Then the training agent turn\n",
    "\t\t\t\t\ttraining_action = self.training_agent.get_action(state_training, epsilon=self.epsilon)\n",
    "\t\t\t\t\tstate_training = self.env_training.do_action(training_action)\n",
    "\n",
    "\t\t\t\t\t# if it is the first step of the observation\n",
    "\t\t\t\t\tif current_skip == skip_frames:\n",
    "\t\t\t\t\t\tdata_point[\"action\"] = training_action \n",
    "\n",
    "\t\t\t\t\tcurrent_skip -= 1\n",
    "\n",
    "\t\t\t\t\tif (state_training.winner is not None) or game_duration >= max_duration:\n",
    "\t\t\t\t\t\tgame_over = True\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tcurrent_skip = skip_frames\n",
    "\t\t\tgame_duration += 1\n",
    "\n",
    "\t\t\tdata_point[\"next_obs\"] = state_training\n",
    "\t\t\tdata_point[\"reward\"] = self.compute_reward(data_point[\"obs\"], data_point[\"next_obs\"], game_over)\n",
    "\t\t\tdata_point[\"done\"] = game_over\n",
    "\n",
    "\t\t\tscore += data_point[\"reward\"]\n",
    "\n",
    "\t\t\tif game_over:\n",
    "\t\t\t\tgame_duration = 0\n",
    "\t\t\t\tscores.append(score)\n",
    "\t\t\t\tscore = 0\n",
    "\t\t\t\tprint(\"game has been reseted and winner was : \", data_point[\"next_obs\"].as_tuple()[2]) # only for debug TODO remove this line\n",
    "\t\t\t\tstate_training, state_to_beat = self.reset()\n",
    "\n",
    "\t\t\tself.memory.store(data_point)\n",
    "\n",
    "\t\t\twith torch.cuda.amp.autocast():\n",
    "\t\t\t\tif len(self.memory) > 1: # No batch size for the moment\n",
    "\t\t\t\t\tsample = self.memory.sample_batch()\n",
    "\n",
    "\t\t\t\t\tloss = self.training_agent.update_model(sample)\n",
    "\t\t\t\t\tlosses.append(loss)\n",
    "\n",
    "\t\t\t\t\tupdate_count += 1\n",
    "\n",
    "\t\t\t\t\tif update_count % self.target_update == 0:\n",
    "\t\t\t\t\t\tself.training_agent._targe_hard_update()\n",
    "\n",
    "\t\t\t\t\tself.epsilon = max(\n",
    "\t\t\t\t\t\tself.min_epsilon, self.epsilon - (self.max_epsilon - self.min_epsilon) * self.epsilon_decay\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\tepsilons.append(self.epsilon)\n",
    "\n",
    "\t\t\tif update_count % log_each == 0:\n",
    "\t\t\t\tself._plot(i, scores, losses, epsilons)\n",
    "\n",
    "\n",
    "\t\t\tif i % reset_agent_each == 0: # reset the to beat agent to the current trained agent\n",
    "\t\t\t\tself.to_beat_agent.brain.load_state_dict(self.training_agent.brain.state_dict())\n",
    "\n",
    "\t\t\tif i % log_each == 0:\n",
    "\t\t\t\tprint(f\"{i} steps done\")\n",
    "\t\t\ti += 1 # TODO : only update i when a step of training is done with the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(BuffedDQNAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(steps=1000, log_each=20, reset_agent_each=100, max_duration=50, skip_frames=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3336df672a544e285a8d6fe4ff0ef23e5aa65345aa81e9136b09e43bc523f0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ReinforcmentLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
