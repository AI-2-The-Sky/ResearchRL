{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bomberman.Environment import Environnement\n",
    "import torch\n",
    "from DQN_Agent import BuffedDQNAgent\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Trainer():\n",
    "\tdef __init__(self, Agent : BuffedDQNAgent) -> None:\n",
    "\t\tself.training_agent = Agent(1)\n",
    "\t\tself.env_1 = Environnement(1)\n",
    "\n",
    "\t\tself.to_beat_agent = Agent(2)\n",
    "\t\tself.env_2 = Environnement(2)\n",
    "\n",
    "\tdef reset(self):\n",
    "\t\t# Player 1\n",
    "\t\tstate_1 = self.env_1.reset()\n",
    "\n",
    "\t\t# Player 2\n",
    "\t\tstate_2 = self.env_2.reset()\n",
    "\t\treturn state_1, state_2\n",
    "\t\t\n",
    "\tdef compute_reward(self, old_state, next_state, game_over):\n",
    "\t\t# TODO\n",
    "\t\twinner = next_state.as_tuple()[2]\n",
    "\t\t\n",
    "\t\tif game_over: # if game is over and player is not the winner, return -1 else 1\n",
    "\t\t\treturn -1 if not winner or winner != self.training_agent.player_num else 1\n",
    "\t\t\n",
    "\t\treturn 0\n",
    "\n",
    "\tdef train(self, steps=10, log_each=1, skip_frames=4, reset_agent_each=10, max_duration=50):\n",
    "\t\tstate_1, state_2 = self.reset()\n",
    "\t\tcurrent_skip = skip_frames\n",
    "\t\t\n",
    "\t\ti = 1\n",
    "\n",
    "\t\t# Use a game duration to force agent to finish the game fast\n",
    "\t\t# (and avoid starting agent which are random to never end playing)\n",
    "\t\tgame_duration = 0\n",
    "\n",
    "\t\twhile i <= steps:\n",
    "\t\t\tgame_over = False\n",
    "\n",
    "\t\t\tdata_point = {\"obs\": state_2} # need [obs, action, reward, next_obs, done]\n",
    "\n",
    "\t\t\twhile current_skip > 0 and not game_over:\n",
    "\t\t\t\twith torch.no_grad(): # Don't compute grad for game playing\n",
    "\t\t\t\t\ttime.sleep(0.01)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t# To_beat_agent turn\n",
    "\t\t\t\t\ttobeat_action = self.to_beat_agent.get_action(state_1)\n",
    "\t\t\t\t\tstate_1 = self.env_1.do_action(5) # put bomb to test game reset TODO replace with tobeat_action\n",
    "\t\t\t\t\t# Then the training agent turn\n",
    "\t\t\t\t\ttraining_action = self.training_agent.get_action(state_2)\n",
    "\t\t\t\t\tstate_2 = self.env_2.do_action(training_action)\n",
    "\n",
    "\t\t\t\t\t# if it is the first step of the observation\n",
    "\t\t\t\t\tif current_skip == skip_frames:\n",
    "\t\t\t\t\t\tdata_point[\"action\"] = training_action \n",
    "\n",
    "\t\t\t\t\tcurrent_skip -= 1\n",
    "\n",
    "\t\t\t\t\tif (state_2.winner is not None) or game_duration >= max_duration:\n",
    "\t\t\t\t\t\tgame_over = True\n",
    "\t\t\t\n",
    "\t\t\tcurrent_skip = skip_frames\n",
    "\t\t\tgame_duration += 1\n",
    "\n",
    "\t\t\tdata_point[\"next_obs\"] = state_2\n",
    "\t\t\tdata_point[\"reward\"] = self.compute_reward(data_point[\"obs\"], data_point[\"next_obs\"], game_over)\n",
    "\t\t\tdata_point[\"done\"] = game_over\n",
    "\n",
    "\t\t\tif game_over:\n",
    "\t\t\t\tgame_duration = 0\n",
    "\t\t\t\tprint(\"game has been reseted and winner was : \", data_point[\"next_obs\"].as_tuple()[2]) # only for debut TODO remove this line\n",
    "\t\t\t\tstate_1, state_2 = self.reset()\n",
    "\n",
    "\t\t\t# TODO : add to replay buffer and train with replay buffer\n",
    "\n",
    "\t\t\tif i % reset_agent_each == 0: # reset the to beat agent to the current trained agent\n",
    "\t\t\t\tself.to_beat_agent.brain.load_state_dict(self.training_agent.brain.state_dict())\n",
    "\n",
    "\t\t\tif i % log_each == 0:\n",
    "\t\t\t\tprint(f\"{i} steps done\")\n",
    "\t\t\ti += 1 # TODO : only update i when a step of training is done with the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(BuffedDQNAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(steps=1000, log_each=100, reset_agent_each=300, max_duration=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3336df672a544e285a8d6fe4ff0ef23e5aa65345aa81e9136b09e43bc523f0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ReinforcmentLearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
